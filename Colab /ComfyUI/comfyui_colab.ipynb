{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbbbbbbbbb"
      },
      "outputs": [],
      "source": [
        "#@title Environment Setup\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = False  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI =-\n",
        "  !git pull\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip install xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dddddddddd"
      },
      "outputs": [],
      "source": [
        "# Checkpoints\n",
        "\n",
        "### SDXL\n",
        "### I recommend these workflow examples: https://comfyanonymous.github.io/ComfyUI_examples/sdxl/\n",
        "\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# SDXL ReVision\n",
        "#!wget -c https://huggingface.co/comfyanonymous/clip_vision_g/resolve/main/clip_vision_g.safetensors -P ./models/clip_vision/\n",
        "\n",
        "# SD1.5\n",
        "!wget -c https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -P ./models/checkpoints/\n",
        "\n",
        "# SD2\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Some SD1.5 anime style\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
        "#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta3/resolve/main/wd-illusion-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# unCLIP models\n",
        "#!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# VAE\n",
        "!wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/\n",
        "\n",
        "\n",
        "# Loras\n",
        "#!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/theovercomer8sContrastFix_sd21768.safetensors #theovercomer8sContrastFix SD2.x 768-v\n",
        "#!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/theovercomer8sContrastFix_sd15.safetensors #theovercomer8sContrastFix SD1.x\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors -P ./models/loras/ #SDXL offset noise lora\n",
        "\n",
        "\n",
        "# T2I-Adapter\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -P ./models/controlnet/\n",
        "\n",
        "# T2I Styles Model\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -P ./models/style_models/\n",
        "\n",
        "# CLIPVision model (needed for styles model)\n",
        "#!wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/clip_vit14.bin\n",
        "\n",
        "\n",
        "# ControlNet\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -P ./models/controlnet/\n",
        "\n",
        "# ControlNet SDXL\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-canny-rank256.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-depth-rank256.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-recolor-rank256.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/stabilityai/control-lora/resolve/main/control-LoRAs-rank256/control-lora-sketch-rank256.safetensors -P ./models/controlnet/\n",
        "\n",
        "# Controlnet Preprocessor nodes by Fannovel16\n",
        "#!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
        "\n",
        "\n",
        "# GLIGEN\n",
        "#!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -P ./models/gligen/\n",
        "\n",
        "\n",
        "# ESRGAN upscale model\n",
        "#!wget -c https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -P ./models/upscale_models/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with cloudflared (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjjj",
        "outputId": "6703f2cc-8591-4bf8-cff4-13747e01881b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_in.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_in.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_in.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_out.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_out.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_out.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_in.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_in.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_in.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_out.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_out.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_out.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_in.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_in.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_in.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_out.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_out.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_out.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "Requested to load SDXLRefinerClipModel\n",
            "Loading 1 new model\n",
            "Requested to load SDXLRefiner\n",
            "Loading 1 new model\n",
            "unload clone 1\n",
            "ERROR diffusion_model.input_blocks.4.1.proj_in.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.proj_out.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.proj_in.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.proj_out.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.7.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "100% 20/20 [00:05<00:00,  3.47it/s]\n",
            "100% 5/5 [00:01<00:00,  3.96it/s]\n",
            "Prompt executed in 50.89 seconds\n",
            "model_type EPS\n",
            "adm 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "missing {'cond_stage_model.clip_l.text_projection', 'cond_stage_model.clip_l.logit_scale'}\n",
            "left over keys: dict_keys(['conditioner.embedders.0.logit_scale', 'conditioner.embedders.0.text_projection', 'conditioner.embedders.1.model.transformer.text_model.embeddings.position_ids', 'cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Requested to load SDXLClipModel\n",
            "Loading 1 new model\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "unload clone 1\n",
            "100% 20/20 [00:04<00:00,  4.19it/s]\n",
            "100% 5/5 [00:01<00:00,  3.95it/s]\n",
            "Prompt executed in 58.00 seconds\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "100% 20/20 [00:04<00:00,  4.21it/s]\n",
            "100% 5/5 [00:01<00:00,  3.97it/s]\n",
            "Prompt executed in 9.19 seconds\n",
            "model_type EPS\n",
            "adm 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "missing {'cond_stage_model.clip_l.text_projection', 'cond_stage_model.clip_l.logit_scale'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Requested to load SDXLClipModel\n",
            "Loading 1 new model\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "unload clone 1\n",
            "100% 20/20 [00:04<00:00,  4.21it/s]\n",
            "100% 5/5 [00:01<00:00,  3.96it/s]\n",
            "Prompt executed in 58.04 seconds\n",
            "got prompt\n",
            "model_type EPS\n",
            "adm 2560\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "got prompt\n",
            "loaded straight to GPU\n",
            "Requested to load SDXLRefiner\n",
            "Loading 1 new model\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_mlp_fc1.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_mlp_fc2.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te1_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_12_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_13_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_14_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_15_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_16_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_17_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_18_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_19_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_20_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_21_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_22_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_23_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_24_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_25_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_26_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_27_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_28_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_29_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_30_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_31_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc1.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc1.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc1.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc2.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc2.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_mlp_fc2.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_k_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_out_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_q_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_v_proj.alpha\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight\n",
            "lora key not loaded lora_te2_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_7_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_input_blocks_8_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_middle_block_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_in.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_in.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_in.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_out.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_out.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_proj_out.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_1_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_2_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_3_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_0_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_in.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_in.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_in.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_out.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_out.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_proj_out.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_1_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_2_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_3_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_1_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_in.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_in.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_in.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_out.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_out.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_proj_out.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_1_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_2_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_3_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_4_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_5_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_6_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_7_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_8_ff_net_2.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn1_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_k.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_out_0.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_q.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_attn2_to_v.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_0_proj.lora_up.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.alpha\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.lora_down.weight\n",
            "lora key not loaded lora_unet_output_blocks_2_1_transformer_blocks_9_ff_net_2.lora_up.weight\n",
            "Requested to load SDXLRefinerClipModel\n",
            "Loading 1 new model\n",
            "Requested to load SDXLRefiner\n",
            "Loading 1 new model\n",
            "unload clone 1\n",
            "ERROR diffusion_model.input_blocks.4.1.proj_in.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.proj_out.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.proj_in.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.proj_out.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.7.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "100% 20/20 [00:05<00:00,  3.58it/s]\n",
            "100% 5/5 [00:01<00:00,  3.94it/s]\n",
            "Prompt executed in 53.35 seconds\n",
            "Requested to load SDXLRefinerClipModel\n",
            "Loading 1 new model\n",
            "Requested to load SDXLRefinerClipModel\n",
            "Loading 1 new model\n",
            "Requested to load SDXLRefiner\n",
            "Loading 1 new model\n",
            "ERROR diffusion_model.input_blocks.4.1.proj_in.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.4.1.proj_out.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.proj_in.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_k.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_v.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight shape '[6144, 768]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.ff.net.2.weight shape '[768, 3072]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_q.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_k.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_v.weight shape '[768, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.input_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.5.1.proj_out.weight shape '[768, 768]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.input_blocks.7.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.7.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.7.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.input_blocks.8.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.input_blocks.8.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.2.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 13107200\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 6553600\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 2621440\n",
            "ERROR diffusion_model.middle_block.1.transformer_blocks.3.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.middle_block.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.3.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.3.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.4.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.4.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.proj_in.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_k.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_v.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn1.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.ff.net.0.proj.weight shape '[12288, 1536]' is invalid for input of size 3276800\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.ff.net.2.weight shape '[1536, 6144]' is invalid for input of size 1638400\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_q.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_k.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_v.weight shape '[1536, 1280]' is invalid for input of size 1310720\n",
            "ERROR diffusion_model.output_blocks.5.1.transformer_blocks.1.attn2.to_out.0.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "ERROR diffusion_model.output_blocks.5.1.proj_out.weight shape '[1536, 1536]' is invalid for input of size 409600\n",
            "100% 20/20 [00:05<00:00,  3.43it/s]\n",
            "100% 5/5 [00:01<00:00,  3.94it/s]\n",
            "Prompt executed in 15.61 seconds\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
        "\n",
        "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  for line in p.stderr:\n",
        "    l = line.decode()\n",
        "    if \"trycloudflare.com \" in l:\n",
        "      print(\"This is the URL to access ComfyUI:\", l[l.find(\"http\"):], end='')\n",
        "    #print(l, end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjj"
      },
      "outputs": [],
      "source": [
        "!npm install -g localtunnel\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
        "\n",
        "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
        "  for line in p.stdout:\n",
        "    print(line.decode(), end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggggggggg"
      },
      "source": [
        "### Run ComfyUI with colab iframe (use only in case the previous way with localtunnel doesn't work)\n",
        "\n",
        "You should see the ui appear in an iframe. If you get a 403 error, it's your firefox settings or an extension that's messing things up.\n",
        "\n",
        "If you want to open it in another window use the link.\n",
        "\n",
        "Note that some UI features like live image previews won't work because the colab iframe blocks websockets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhhhhhhhh"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import socket\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "  print(\"to open it in a window you can open this link here:\")\n",
        "  output.serve_kernel_port_as_window(port)\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}